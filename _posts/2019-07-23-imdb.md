---
title: "Underlying Themes of Movies Through Time"
date: 2019-07-03
tags: [data wrangling, data mining, clustering]
header:
excerpt: "by Cedric Conol, Rosely Pena, Bok Torregoza"
mathjax: "true"
---


<img src="/images/imdb/banner.png"></img>

<div style="-webkit-column-count: 3; -moz-column-count: 3; column-count: 3;">
</div> 

## Executive Summary

Internet Movie Database, commonly known as IMDb, is an online database of information related to films, television programs, home videos and video games. The study aims to determine the different themes of movies through time using unsupervised clustering. Movie titles, description, genre, ratings, directors, country of origin and languages were scraped from IMDb using Scrapy, a Python Web Scraping Framework, and were stored in a SQLite3 database. The analysis focused only on American movies produced between 1980 and 2019, filtered by ratings and grouped into four (4) decades: 1980-1989, 1990-1999, 2000-2009 and 2010-2019. Movies with ranking of 6.0 and above were classified as ‘above average’ and with ratings below 6.0 were tagged as below average. Descriptions of each movie were vectorized using term-frequency inverse document frequency (TF-IDF) weighting and dimensionalities of the design matrices were reduced to n components that would explain at least 80% of the variance using Latent Semantic Analysis (LSA). Unsupervised clustering using the Ward Method was used to generate clusters per decade. Results show that movies set in New York city consistently received above average ratings throughout the four decades. Movies based on true stories had significantly increased in ratings in the past two decades (2000-2019) while movies about violence, deaths and killings received lower ratings in three decades, specifically 1980-1989, 2000-2009 and 2010-2019. Findings of the study can be used by producers to determine what type of movies to develop, or create movies with themes that have never been explored however, addition of additional features such as movie keywords should be done to generate more significant results.



## Introduction

### IMDB: The Internet Movie DataBase

Internet Movie Database, commonly known as IMDb, is an online database of information related to films, television programs, home videos and video games, and streaming content online including cast, production crew and personal biographies, plot summaries, trivia, fan reviews and ratings. The movie and talent pages of IMDb are accessible to all internet users, but a registration process is necessary to contribute information to the site.

The site started with a list of actors/actresses and movies and had been expanded to include additional categories of filmmakers and other demographic material as well as trivia, biographies, and plot summaries. Most data in the database is provided by volunteer contributors. IMDb claims itself to be the "world's most popular and authoritative source for movie, TV and celebrity content, designed to help fans explore the world of movies and shows and decide what to watch."

### Movie Industry

The Film Industry has been evolving since its birth during the 19th century. The global film industry is worth USD 136 billion as of 2018 and the global box office is worth USD 41.7 billion. Despite its valuation, the Film Industry is highly capital and labor intensive, making investment risky especially for producers.

Hollywood was the birthplace of movie studios in the US. The rise of Hollywood enabled lowered costs on studios because creative inputs had less down-time, needed to travel less, could participate in many try-outs to achieve optimal casting and could be rented out easily to competitors when not immediately wanted. The earliest and most affluent film companies were Warner Brothers Pictures, Paramount, RKO, Metro Goldwin Meyer, and 20th Century Fox, each of whom owned their own film production sets and studios.

Like other major industries, such as the automobile, electricity, chemicals and the airplane, the Film Industry evolved through time and embraced technology in order to hold the attention of the audience better. This can be evident in the evolution of shots, motion and storytelling. In the present era, pressure on studio executives to minimize cost while creating hit movies is on the rise.

### Problem Statement

Film industry is adverse to risk because movies are so expensive to make. Sequels of popular films are made due to guaranteed audience only or to maximize the return on that initial investment and remakes are produced due to the original film's recognition value.

In order to eliminate the risks in producing movies, this study aims to identify how the underlying themes in movies change over time. By classifying the movies per decade into above and below average, themes which had caught the interests of the viewers and those which did not pay out can be revealed.

**Scopes and Limits**

The study will focus on the following:
	
- Movies produced in USA
- Movies will be divided into four (4) decades: 1980-1989, 1990-1999, 2000-2009, 2010-2019
- For each decade, only the top 25% movies based on number of user votes will be considered
- Movies with rating above 6.0 will be considered as above average while movies with rating below 6.0 will be tagged as below average.

## Methodology

To meet the objectives of the study, movies in each decade from 1980 onwards are classified first into **above average** and **below average** titles. Above average titles are those movies which has a rating of at least 6.0, below that are considered below average. For each decade, above and below average titles will be clustered separately. Clustered results from above average titles will then be compared with above average clusters from other decades, same goes for below average titles.

![flowchart](/images/imdb/flowchart.png)
Schematic of the data pipeline for the analysis of the underlying themes of IMDb titles from 1980-2019.

The movie's **description** was selected as the feature that will be used to cluster titles together. It will undergo cleaning in the text pre-processing stage where punctuations will be removed, text will be converted to lower case and words will be reduced to their root form. It will then be converted to their vector representation using TF-IDF, something that algorithms will be able to use. It is expected that some vectors in the resulting matrix will contain a lot of zeros as movie descriptions will not contain all the words in the document. Euclidean Distance will not be a good distance metric to use since straight line distances between vectors with a lot of zeros will be considered similar even if they don't contain similar words so Cosine Similarity was used. Cosine similarity distance metric won't have any difficulty differentiating vectors with a lot of zeros. 

The dimension of the design matrix is too large and using the entire matrix will include noisy words and will be too computationally expensive when clustering. Latent Semantic Analysis (LSA) will be used to reduce the dimension of the design matrix to a size that will explain at least 80% of the variance.

Agglomerative clustering using Ward's method was used to cluster the titles. In the sensitivity analysis stage, the number of clusters is find-tuned such that the resulting clusters make sense to meet the objectives of the study.

Details of each method will be discussed on each section.

## Data Collection

This process involves extraction and storage of data from Internet Movie Database (IMDb). Data will be scraped using library and will be stored in a SQLite 3 database. As of May 2019, IMDb.com has 5.9 million titles which was trimmed down to 250 thousand titles after applying pre-filters.

### Scraping

The data was collected by scraping [imdb.com](https://www.imdb.com/) using a free and open source framework for web scraping, [`scrapy`](https://scrapy.org/). Scrapy is one of the most popular and powerful Python scraping libraries; it takes a "batteries included" approach to scraping, meaning that it handles a lot of the common functionality that all scrapers need so developers don't have to reinvent the wheel each time. The data was scraped on **July 6, 2019**.

The first step to scraping is by inspecting the actual website and using CSS selectors to get the data needed. For example, when extracting the title of the page, we used the CSS selector `h1`, like so:


>```
>from scrapy.spiders import CrawlSpider
>
>class MovieSpider(CrawlSpider):
>    def parse_movie_detail_page(self, response):
>        data = {}
>        data['title'] = response.css('h1::text').extract_first().strip()
>        yield data
>```

Movie information between **1980** and **July 6, 2019** were collected from web scraping and is stored in a file as a list of json strings.

<img src="/images/imdb/scrapedoutput.png" width=400>

### Data Storage

JSON files are then collected and stored into a SQL Database using `sqlite3` - a lightweight disk-based database library for python.

The database is named `imdb.db`. The database schema is shown below:
![dbschema](/images/imdb/dbschema.png)

The database contains **504, 787** titles.


## Data Preparation  

This process involves examination of the data using Exploratory Data Analysis (EDA), data cleaning and wrangling in order to transform the data to an acceptable format for analysis and data mining, further processing of the data to be transformed into a format acceptable by the clustering algorithm.

### Data Description

The collected data contains the following information:

| Name on Database | Data Type | Description |
|:----------------:|:----------|:------------|
|title_id|  string         |  IMDb's unique identifier for movies and videos  |
|title            |  string         |  Official movie/video title          |
|certification    |  string         |  Film's suitability for certain audiences based on its content [(link)](https://en.wikipedia.org/wiki/Motion_Picture_Association_of_America_film_rating_system)           |
|img_url   | string   | URL of the film's poster    |
|imdb_url   |  string  | Film's IMDb URL   |
|tagline   | string   | Film's tagline/slogan  |
|description   | string   | Short description of the film   |
|runtime_min   |  float  | Length of film in minutes   |
|actors   | string   | Name of actor/actress   |
|country   | string   | Film's country of origin   |
|director   | string   | Name of film director   |
|genre   | string   | Film's genre   |
|languages   | string    | Language used in the film   |
|userrating    | float   | Film's IMDb user rating in a scale of 1-10  |
|votes    | int   | Film's number of IMDb user votes   |
|metascore    | int   | Film's rating based on reviews provided by metacritic.com ina scale of 0-100  |

### Exploratory Data Analysis

EDA was done prior to data wrangling. This section will show relevant statistics and basic information for the entirety of the dataset. Performing initial investigations on data may uncover patterns, spot anomalies, and validate assumptions with the help of summary statistics and graphical representations.

**Dataset Overview:**

Checking first the count and completeness of all the dataset's features.
>```
RangeIndex: 116245 entries, 0 to 116244
Data columns (total 5 columns):
title_id       116245 non-null object
title          116245 non-null object
votes          67331 non-null float64
description    116245 non-null object
userrating     67331 non-null float64
dtypes: float64(2), object(3)
memory usage: 4.4+ MB
>```
>```
Nulls:
title_id           0
title              0
votes          48914
description        0
userrating     48914
>```

Columns `title_id`, `titles` and `description` each has 116245 number of rows and does not contain null values. `votes` and `userrating` are the only numerical columns in the dataset, both has 67331 rows and 48914 null values. Everytime an IMDb user enters a rating for the film, his rating is counted as a vote which explains the similarity in the count and completeness of votes and user ratings. Null values will be excluded in the analysis to avoid any negative impacts in the resulting clusters.


**User Rating statistics**:

IMDb users can rate the films in a scale of 1 to 10. This section explores the distribution of the ratings among the films.
>```
count	  67331
mean	   6.068789
50%        6.2
min	    1
max	    10
>```
<img src='/images/imdb/userrating.png' width="500"></img>

The histogram shows that the distribution of user ratings follows a Gaussian distribution with mean at 6.06. A number close to the mean will be used as the value which separates films which are above average and below average. The mean is 6.07, rounding that off to the nearest integer, so the rating which will separate the films will be 6.0. Greater than or equal to 6.0 is above average below it is below average.


**Votes statistics**:

This section explores how much user votes the films got.
>```
count	  67331
mean	   8.728806e+03
min	    5
max	    2.109494e+06
>```
<img src='/images/imdb/votes.png' width="500"></img>

There is a huge difference between the mininum and maximum number of votes which was visualized as well on the boxplot above. This huge variation in the number of votes can affect the quality of classifying above and below average movies as some films might have a high rating of 10 but have only 5 votes. Hence, the analysis will focus only on the top 25% of films in terms of the number of votes. This will ensure that the analysis will only include the most voted or popular movies. 


**Description column statistics**:

This section looks into the relevant statistics of the feature of interest. Description feature shortly describes the film, it usually is a single statement and does not tell the entirety of the movie. A wordcloud will be used to visualize what the most common words are in the films' descriptions.
>```
count	  116245
unique	 115592
top	    Official music video for
freq	   24
>```
<img src='/images/imdb/wordcloud.PNG' width="500"></img>

From the wordcloud above, the words "music" and "videos" are quite prevalent on the description, these are not movies but will still be considered in the analysis as it is still classified as a film. Movie related words also came out as some of the most common words as expected, these are words like "life", "love", "family" and "story".


### Data Wrangling

This process involves transformation of the text dataset to a uniformed format to be suitable for analysis. The following steps will be discussed thoroughly in this section:

1. Transform to lower case
2. Removal of punctuations
3. Lemmatization

Due to the relatively large number of titles collected, some prefilters are applied which will help reduce the computational requirement without sacrificing the quality of the resulting dataset.

To reduce the number of titles the following pre-filters are applied:
1. Movies produced in USA
2. Movies from 1980-2019
3. Getting only the top 25% of titles in terms of user votes each decade
4. Classified the titles into above average (ratings >= 6.0) and below average (ratings < 6.0)

This code snippet will fetch movie details from USA for the decade 1990-1999 from the `imdb.db` database and store it in a pandas.DataFrame object, `df`.

>```
>import sqlite3
>
>conn = sqlite3.connect('imdb.db')
>df = pd.read_sql('''SELECT titles.title_id, title, votes, description, rating.userrating FROM titles 
                  JOIN countries ON countries.title_id=titles.title_id
                  JOIN rating ON rating.title_id=countries.title_id
                  WHERE country="USA" AND titles.year>=1990 AND titles.year<=1999
                  AND description IS NOT NULL''', conn)
>```

`sqlite3.connect('imdb.db')` will open a database connection to a database named imdb.db which was created earlier in the data storage section. `pd.read_sql` takes in a SQL query and a database connection as parameters and returns the result of the SQL query as a pandas.DataFrame object.

The dataframe will be further reduced by only selecting the top 25% of the movie titles in terms of the number of user votes. This step ensures only the most popular movies in each decade will be considered. The code snippet below will perform the aforementioned steps.

>```
>votes_75 = df['votes'].quantile(.75)
>df = df[df['votes'] > votes_75]
>```

`pandas.DataFrame.quantile` takes in a given quantile and returns the values. Using 0.75 as the parameter means 75% quantile.

Movie descriptions of the resulting dataframe will then undergo data cleaning.
Here's an example of a movie description from the movie Titanic:

>“A seventeen-year-old aristocrat falls in love with a kind but poor artist aboard the luxurious, ill-fated R.M.S. Titanic”

Movie titles will be clustered using the similarity of their description, thus descriptions must be uniformly transformed to the same form. This is achieved using text pre-processing.

Languages we speak and write are made up of several words often derived from one another. When a language contains words that are derived from another word as their use in the speech changes is called **Inflected Language**. The degree of inflection may be higher or lower in a language. As you have read the definition of inflection with respect to grammar, you can understand that an inflected word(s) will have a common root form, which can be achieved by **stemming** or **lemmatizing**.

Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language while lemmatization, unlike stemming, reduces the inflected words properly ensuring that the root word belongs to the language. Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma. Although lemmatizing takes longer than stemming, word lemmatization gives an actual word using the build it WordNet corpus which is more helpful for this study to see the underlying themes in the clusters.

The following code snippet will lemmatize the words in the pandas.Series, `df0`.

>```
>from nltk.stem import WordNetLemmatizer
>
>lemmer = WordNetLemmatizer()
>df1 = [' '.join([lemmer.lemmatize(word, pos='v') for word in
        text.lower().replace("\n", " ").split(" ") if len(word) > 1])
        for text in df0]
>```

`lemmer` is an instance of the `WordNetLemmatizer` class. It has an attribute `.lemmatize` that takes in a word and lemmatizes it based on the parameter `pos` which was set on the above example as `v` or verb. Verbs like "falls" will be lemmatized as "fall"

Text pre-processing steps include, **changing the case to lower case, removing punctuations and lemmatizing** the words in the description. When these text pre-processing steps are applied to the example movie description above it will give the following result:

>“seventeen year old aristocrat fall love kind poor artist aboard luxurious ill fat rms titanic”

For the example above, stop words are also removed. **Stop words** are words which do not contain important significance. Usually, these words are filtered out because they return a vast amount of unnecessary information. Mostly they are words that are commonly used in the English language such as 'as, the, be, are' etc.

After the cleaning process, movie titles are then classified as above average (ratings >= 6.0) and below average (rating < 6.0).

>```
>df_above = df[df['userrating']>5.9] # 6.0 and above
>df_below = df[df['userrating']<6.0] # 5.9 and below
>```


## Data Mining

The objective of this stage is to transform the documents into its vector form and reduce noise. Vectorization converts textual data into numerical format, which is suitable for clustering algorithms. The output matrix would have high dimensions (number of rows and columns) thus, dimensionality reduction using Latent Semantic Analysis will be used to reduce noise in the data and save computational power at the same time.

### TF-IDF Vectorization

The documents will be vectorized using Term Frequency - Inverse Document Frequency (TF-IDF). The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.

The inverse document frequency of a word is given by

$$\text{idf}_w = \log{\frac{n}{n_w}},$$

where $n$ is the number of documents in the corpus and $n_w$ is the number of documents the word appeared in.
As mentioned above, a word occuring in all documents won't be as useful as a word that occurred in fewer documents especially if they occur only in relevant documents. We can use the idf as a weight so that popular words are deemphasized and this is known as the TF-IDF model.

sklearn's `TfidfVectorizer` will be used to vectorize the data. Parameters used are `stop_words`, `token_pattern` and `ngram_range`.

>`stop_words = 'english'`: this will remove stop words in the English language like 'the', 'and' and 'is'
>
>`token_pattern = r'[a-z]+'`: with this parameter, a token is a series of 1 or more alphabetical letters
>
>`ngram_range = (2,2)`: the tuple in this parameter represents the lower and upper boundary of the range of n-values for different n-grams to be extracted, with this parameter set to (2,2) means it is vectorizing text with a bi-gram

>```
>tfidf_a = TfidfVectorizer(stop_words='english', 
                          token_pattern=r'[a-z]+',
                          ngram_range=(2,2))
>
>bow_above = tfidf_a.fit_transform(df_above['description'])
>nonzeros_above = bow_above.sum(axis=1).nonzero()[0]
>bow_above = bow_above[nonzeros_above]
>```


### Choosing distance metric

The resulting matrix is a sparse matrix where each vector contains a lot of zeros. Cosine similarity is used because it is better at catching the semantic of each text, the direction the text points can be thought as its meaning so texts with similar meanings will be similar. 

Many distance-based data mining applications lose their effectiveness as the dimensionality
of the data increases. For example, a distance-based clustering algorithm may group unrelated
data points because the distance function may poorly reflect the intrinsic semantic
distances between data points with increasing dimensionality. As a result, distance-based
models of clustering, classification, and outlier detection are often qualitatively ineffective.
This phenomenon is referred to as the **“curse of dimensionality,”** a term first coined by
Richard Bellman.

>```
>dist_a = 1 - cosine_similarity(bow_above.toarray())
>```

`sklearn.metrics.pairwise.cosine_similarity` computes the cosine similarity of vectors, it ranges from -1 to 1. This value will be subtracted from 1 to make sure that there are no negative value distances, it will then have a value ranging from 0-2.


### Dimensionality Reduction

At this point, the fact that the more features we have, the more difficult it is to visualize and process the data is already established. It is important to note that the lesser the number of dimensions, the easier it is for computers to process.

**The Curse of Dimensionality**

Adding features without adding samples leads to increase in dimensionality of the data set and making it sparser. Because of such sparsity, it is easier to find a perfect solution for the model that results to over-fitting.

**Latent Semantic Analysis**

Since the data in this study is composed of text, Latent Semantic Analysis (LSA) was chosen as the method for reducing the dimension. LSA is an application of Singular Value Decomposition (SVD) method for the text domain. The sparsity of the data matrix results in a low intrinsic dimensionality. Therefore, in the text domain, the reduction in
dimensionality from LSA is rather drastic. For example, it is not uncommon to be able to
represent a corpus drawn on a lexicon of 100,000 dimensions in fewer than 300 dimensions.

LSA is very similar to PCA except that Truncated SVD uses the data matrix while PCA uses the covariance matrix. LSA can be implemented using `sklearn.decomposition.TruncatedSVD`, this will accept the desired number of components that it will use to reduce the dimension of the data matrix using the `n_components` parameter. For this study, the number of components was chosen such that it will explain at least 80% of the variance.

>```
>comp_a = int((dist_a.shape[0]*0.8)//1)
>lsa_above = TruncatedSVD(comp_a)
>X_above = lsa_above.fit_transform(dist_a)
>```


## Clustering  

Clustering involves grouping of the transformed dataset based on features, aggregating the data points according to similarity measures. Different algorithms are used and the choice of algorithm depends on the characteristics of the data set and the objective. Agglomerative clustering was the chosen method for this study.

### Agglomerative Clustering: Ward's Method

Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. It is non-parametric which means it does not need a prior assumption on the number of clusters before performing the actual clustering. One strategy of hierarchical clustering is the bottom-up approach or the Agglomerative Clustering method. 

One of the most common methods of agglomerative clustering is Ward’s Method. It is also known as Minimum Variance Method. It is popular in the field of linguistics and is expected to produce compact and even-sized clusters. Ward's method is more robust to noise due to the use of multiple linkages in the distance computation as compared to single and complete linkages.

This code snippet will create a linkage matrix using Ward's method:

>```
>from scipy.cluster.hierarchy import ward, dendrogram
>
>linkage_matrix = ward(X_above)
>```

Plotting the resulting matrix as a dendrogram using `dendrogram` will provide a better look as to how the number of clusters are determined. `orientation='top'` plots the root at the top, and plot descendent links going downwards. `no_labels` was set to True so that no labels appear next to the leaf nodes in the rendering of the dendrogram thus making the plot a lot more cleaner. `truncate_mode='level'` is used to condense the dendrogram based on the number of levels specified in `p` parameter, setting it to 7 gives a more readable dendrogram.

The class `imdbCluster` created for this study has an attribute `plotdendrogram` which will take the `design_matrix` and `level` as parameters. This will provide a dendrogram with a horizontal line drawn on the `level` parameter, it will also print the range of values for `level` that has the biggest drop in vertical distance - which tells you the number of clusters which are optimal for the dataset. This can be easily done in an `imdbCluster` object using the following snippet

>```
>dendrogram_plot = imdbobj.plotdendrogram(X_above, 2.5)
>```
<img src='/images/imdb/dendrogram.png' width="500"></img>

It can be seen that the red line crossed 4 vertical lines which corresponds to the number of clusters that will be used. `topwordspercluster` of the `imdbCluster` class gives out the top words in each resulting cluster, `clusteredtitles` will give a dataframe with the cluster groups as columns and film titles as the values.

>```
>k=4
top_words, indices, _ = imdbobj.topwordspercluster(dendrogram_plot, k, bow, features)
top_words.head()
>```
<img src='/images/imdb/topwords.png' width="400"></img>
>```
>clustered = imdbobj.clusteredtitles(design_matrix_original, indices, k)
>```
<img src='/images/imdb/clustered.png' width="800"></img>


## Results

This section provides a summary of the clustering results and discussion of insights generated from each decade.

### Theme of the 80’s

**Above Average Movies**

<img src='/images/imdb/clustered.png' width="800"></img>

There are four resulting clusters under this category. One of the two less dominant cluster is about Superman II, and the other is about music videos of Michael Jackson. In the more dominant clusters, one is about life and stories in New York, and the other is about action themes that can either be futuristic and cosmic, war and murder or detective stories.

**Below Average Movies**

<img src='/images/imdb/below80.png' width="800"></img>

There are four resulting clusters in this category. The movies were grouped under the themes school and students, mysterious disappearance, serial killings, horror stories about women, stories about young women or girls and killing sprees. Some popular movies like Rambo III , Conan the Destroyer and Superman III are part of the last cluster which the theme is about killing spree.

### Theme of the 90’s

**Above Average Movies**

<img src='/images/imdb/above90.png' width="800"></img>

The resulting clusters of this decade is more defined than the previous. Hit love stories like Titanic, Pretty Woman and You’ve Got Mail are included in the cluster about falling in love. The iconic Schindler’s List and the award-winning Fatherland are included in the cluster about World War II. Meanwhile, thrillers like Se7en, The Silence of the Lambs and The Bone Collector are part of the cluster about catching a serial killer. The Shawshank Redemption and Forrest Gump are among the top movies in a cluster about a young man or a young boy. It is worth noting that there is another cluster about life and stories in New York in this decade, like in the previous one. It is in this cluster that movies like the iconic comedy sequel Home Alone 2 and the thrilling action Die Hard with A Vengeance belong.

**Below Average Movies**

<img src='/images/imdb/below90.png' width="800"></img>

There are four resulting clusters. There is also a cluster about life in New York, where the action Money Train and the monster Godzilla belong. In addition, there is a cluster about young boys where comedy films centered on a boy, like Home Alone 3 and Richie Rich belongs. A cluster about stories that revolves on a single man like Judge Dredd, Batman Forever, Rocky V and the Nutty Professor was also formed. Lastly, a cluster of movies set on small towns also surfaced.

### Theme of the 2000s

**Above Average**

<img src='/images/imdb/above00.png' width="800"></img>

A cluster about music videos surfaced in this group. In addition, action films that revolves around young man and woman were also classified as part of one cluster. It is in this cluster that the legendary trilogy of The Lord of The Rings belong. True stories also came in as one cluster, where inspiring real-life story Seabiscuit belongs. It is also interesting to note that there is still another cluster about life in New York, where I am Legend and The Taking of Pelham 123 are among those who top this list.

**Below Average**

<img src='/images/imdb/below00.png' width="800"></img>

Stories about school is one cluster in this category, together with stories about falling in love and young man or woman. The notable cluster here is the one about crime and detectives and serial killings which are part of the above average clusters in the previous decade.

### Theme of the 2010's

**Above Average**

<img src='/images/imdb/above10.png' width="800"></img>

The current decade is much the same as the previous in terms of the underlying themes of above average movies. Real life stories like The Wolf at Wall Street and Captain Phillips belong in the cluster of true stories. The cluster on stories about young man includes movies like Inception and Shutter island. Another interesting result is the presence of the cluster about life in New York, which has been present across all decades. In this cluster movies like Spider-Man Homecoming, The Other Guys and Date Night belong.

**Below Average**

<img src='/images/imdb/below10.png' width="800"></img>

In this category, movies about life in New York is also present. In addition, themes about high school life surfaced, where popular movies like Death Note and The Twilight Saga: Eclipse belong. Lastly, a cluster about stories that revolves around women was formed.

## Highlights of the Study

1. Various films from romance, action, sci-fi were set in NYC. It may be because there are numerous iconic sites in the city, or the city is so diverse it fits as a good backdrop for any genre. It is also interesting to note that many independent films today are also set in New York.
2. Movies about crime were hit in the 90’s but the interest died down in the next two decades, 2000 up to 2019, it is remarkable that the homicide rate in the US follows the same pattern. There are 9.8 per 100,000 inhabitants between 1990 to 1999. It dropped to 5.7 in 2000 to 2009 then decreasing to 4.9 in the current decade.
3.  Movies on superheroes started in 1980s and increased popularity up to present.
4. Books belonging to the New York Times Bestseller list like The Fault in Our Stars (2014), Harry Potter, Lord of the Rings Trilogy have also been translated to movies in the past two decades (2000-2019)

## Conclusions

The study was able to achieve its objective to indentify the different trends in the film industry through decade. Insights generated strongly suggest that events happening in the society in a decade have a big impact on the themes of the movies. Results also reflects the way of life per decade which is evident on films produced in the past two decades (2000-2019) that have incorporated technology in the production. 

Findings of the study also explain why some remake and sequels of movies produced in the 1990s. Having the largest number of clusters in the above average rating, the decade had indeed produced a large number of good quality movies. At present, this has been the trend started by The Walt Disney Company to ensure profit given that these films had secured a number of audience. 

In a way, findings of the study tell about the preferences of the consumers that can be used to help businesses attract customers. Given that the film industry is both labor and capital intensive, the insights generated in this study can help producers identify what types of movies to produce to attract viewers. 

The imbalanced clusters obtained on each decade may have been affected by outliers. One of the limitations of agglomerative clustering is it considers outliers as actual clusters since the algorithm starts with singletons. Further cleaning and filtering of the data is recommended. 

## Recommendations

To further improve the results of the study, the following recommendations are suggested:


	
- Removal of music videos in the data set since it was observed that these videos tend to cluster in one cluster per decade.
- It is also recommended to expand the scope of the study by including titles produced from other countries like India and China, the top two (2) producers of films in the world.
- Expansion of features to include movie keywords is highly recommended to clearly define the clusters.
- For future studies, the methods used in the study is recommended to be applied to TV Series or Philippine movies.

## Acknowledgment

The success and final outcome of this project required a lot of guidance and assistance from the MS Data Science faculty particularly Prof. Erika Legara, Prof. Christian Alis and Engr. Eduardo David Jr. who granted permission to use all required  equipment and the necessary materials to complete the tasks.  

Deep gratitude is also given to AIM's Analytics, Computing, and Complex Systems Laboratory for providing the opportunity to do the project work using the laboratory's supercomputer.  


## References

[1] [Stemming](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)

[2] [TF-IDF](https://www.tidytextmining.com/tfidf.html)

[3] [Hierarchical Clustering](https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec)

[4] [Ward's Method](https://www.statisticshowto.datasciencecentral.com/wards-method/)

[5] Aggarwal, C. (2015) Data Mining - The Textbook

[6] https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html

[7] https://medium.com/@cxu24/why-dimensionality-reduction-is-important-dd60b5611543

[8] Mao Xingliang, Li Fangfang, "Clustering of Short Text in Micro-blog Based on K-means Algorithm" (2018)

[9] Lanlan Cheng ; Yueheng Sun ; Jinghui Wei, "A Text Clustering Algorithm Combining K-Means and Local Search Mechanism" (2009)

[10] Xiaoli Wang ; Ying Li ; Meihong Wang ; ZiXiang Yang ; Huailin Dong, "An Improved K_Means Algorithm for Document Clustering Based on Knowledge Graphs" (2018)

[11] Yiyang Wang, Li Wang, Jing Qi, Zhong Qian, Bo Xu, Chao Lei, Yuexiang Yang, Huali Cai, "Improved Text Clustering Algorithm and Application in Microblogging Public Opinion Analysis" (2013)
